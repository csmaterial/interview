### 机器学习算法
##### 1. 线性模型

1. LR 的推导
	- 线性回归目标函数：1.均方误差 2.[似然损失](https://blog.csdn.net/jshazhang/article/details/80487825)  。 提示：似然函数：![1](https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D303/sign=8d0989b0d2f9d72a1364161de72b282a/2fdda3cc7cd98d1096ace1be2a3fb80e7aec90c1.jpg) 。
	- Logistics Regression目标函数：提示：sigmoid函数，代入z。![2](https://latex.codecogs.com/gif.latex?y&space;=&space;\frac{1}{1&plus;e^-^z})
	- 优化方法：
		- 梯度下降
		- 牛顿法
	- LR 为什么用 sigmoid 函数？ 这个函数有什么优点和缺点？为什么不用其他函数？
		- 优点：
			1. 近似**单位阶跃函数**（将输入值转化为一个接近1或0的输出值），且**单调可微**。
			2. 直接对**分类可能性**进行建模，无需事先**假设数据分布**，避免了假设分布不准确所带来的问题。得到近似概率预测。
			3. 通过**最大熵原则**推出的这个 f，就是sigmoid。熵越大不确定度越大。所以大家可以想象到，均匀分布熵最大，因为基本新数据是任何值的概率都均等。
		- 缺点：
			1. sigmoid**过饱和**、丢失了梯度。sigmoid神经元的一个很差的属性就是神经元的活跃度在0和1处饱和，它的**梯度**在这些地方接近于**0**。[sigmoid缺点](https://blog.csdn.net/csuhoward/article/details/52538296)
			2. sigmoid的输出不是**零中心**的。

	
2. [模型对比](https://blog.csdn.net/sinat_32043495/article/details/79681177)
	- LR 和线性回归区别？线性回归可以用来做正负分类么？
		1. 经典线性模型的优化目标函数是**最小二乘**，而逻辑回归则是**似然函数**。线性回归中θTX为预测值的**拟合**函数；而在逻辑回归中θTX=0为**决策边界**
		2. 可以，算出regression的分数后**取sign**就好了。![1](https://upload-images.jianshu.io/upload_images/3243632-2db3248bf05c26b7.png?imageMogr2/auto-orient/) 这个图是什么意思呢？线性回归和线性分类的主要区别就是错误衡量标准，这张图显示了 err（sqr）是err（0/1）的上限，所以，如果err（sqr）在我们接受的范围内，那err（0/1）必然也可以接受。（这个思路很重要）。嗯哼~我们又找到了一个err（0/1）的上限，比VC bound更loose的上限。做回归效率更高，把err（0/1）换成err（sqr），相当于用上限的loose换取了efficiency！
	- LR 和 SVM 区别？
		1. LR的优势在于其输出具有**概率**意义，而支持向量机的输出不具有概率意义。欲得到概率输出需特殊处理。
		2. SVM自带**结构风险**最小化，LR则是**经验风险**最小化：在假设空间、损失函数和训练集确定的情况下，经验风险最小化即最小化损失函数。结构最小化是为了防止过拟合，在经验风险的基础上加上表示模型复杂度的正则项。
		3. SVM会用**核函数**而LR一般不用核函数，使用对率损失函数L来替代损失函数，则几乎得到了LR。该情况下，支持向量机与LR的优化目标相近，通常情况下性能也相当。
		4. SVM的处理方法是只考虑**support vectors**，也就是和**分类最相关的少数点**，去学习分类器。而逻辑回归通过**非线性映射**，大大**减小**了离分类平面**较远的点的权重**，相对**提升**了与分类最相关的数据点的权重。
		5. hinge损失有平坦零区域，使得解具有稀疏性，而LR是光滑的单调递减函数，不能得到类似支持向量的概念，因此LR依赖于更多的训练样本，其预测开销更大。

	- LR 和最大熵模型的关系？
	
3.模型实现
	- [并行实现](https://blog.csdn.net/zhoubl668/article/details/19612215/)

##### 2. SVM
1) SVM 的推导</br>
	- 目标函数：最大间隔。</br>	
		1. [原始问题与对偶问题](https://blog.csdn.net/sunshine_in_moon/article/details/51321461) 。对偶问题的好处：拉格朗日乘子法其实就是把原来的**有约束**的优化问题转化为了**无约束**的优化问题，并且成功的把约束条件包含在了这个无约束的优化问题中。
		2. KKT 条件
		3. 合页损失函数是针对原始问题还是对偶问题？
	- 优化方法：
		1. SMO
		2. SMO 算法的时间复杂度和空间复杂度？（结合机器学习实战分析，最坏情况下复杂度是O(训练样本数的平方)）
	- SVM 怎么防止过拟合
		- 松弛变量和正则
		 
	
2) 其他问题
	
	- SVM 为什么不适合处理大数据？SMO 也不行？为什么适合小样本多特征？LR 适用什么情况？
		- 分为两个层面，一个层面是，svm在**分类效果**上是否适合大规模数据，SVM在小样本训练集上能够得到比其它算法好很多的结果，是因为其本身的优化目标是**结构化风险最小**，而不是经验风险最小，因此，通过margin的概念，可以得到对数据分布的结构化描述，减低了对数据规模和数据分布的要求。另外一个层面是，svm对于大规模数据训练的**运算量**是否太大而无法使用，在这个方面，SVM相对其他分类器没有显著的优势。因为统计学习的核心问题是样本不足时如何得到泛化能力很强的模型，但对于大规模学习来说，障碍往往在于算法的算力不足，不是数据不够，所以也可以说传统的统计学习方法(不只是SVM)就存在着最坏情况下，复杂度是**O(训练样本数的平方)** 的情况。相比而言，就更倾向于与其优化它，不如去优化一些理论更鲁棒的方法（比如GP classification），或者直接就用计算时间较低的暴力非线性方法（如random forest，这是O(nlogn)的方法），[LR 与SVM比较](https://blog.csdn.net/yan456jie/article/details/52524942)
		

		 

##### 3. 树模型
1. 基础决策树模型：ID3，C4.5，CART。 
决策树学习的关键，是如何选择最优划分属性。我们希望分支结点所包含的**样本**尽可能属于**同一类别**，即结点**纯度puriy**越来越高。信息熵越小，D的纯度越高。
	- 分裂节点的选择
		- 信息增益：信息增益越大，意味着使用属性a来进行划分得到的**纯度提升**越大
		- 信息增益比
		- 基尼指数：Gini反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini越小，数据集D纯度越高。
	- 决策树剪枝
		- [CART 树剪枝（递归的方法）](https://blog.csdn.net/zhengzhenxian/article/details/79083643)
	- 决策树损失函数
		- 正则化的最大似然怎么解释？
		- [决策树怎么处理连续值？](https://app.yinxiang.com/shard/s43/nl/7332660/83860de1-b7a9-418e-82b3-631209bb446d?title=%E5%86%B3%E7%AD%96%E6%A0%91)
2. 集成学习
	- bagging：随机森林
		- 随机森林怎么防止过拟合？
		
	- boosting
		- Adaboost
		
		- GBDT
			- GBDT
				- GBDT 原理？加法模型
				- GBDT 分裂节点的选择？
				- GBDT 怎么并发？
				- GBDT 为什么用梯度拟合残差？
				
			- XGBoost
				- XGBoost 目标函数？
				- XGBoost 怎么做并行？
				- XGBoost 与 传统 GBDT 区别？
				- XGBoost 对特征缺失敏感么？对缺失值做了什么操作？哪些模型对缺失值敏感？哪些不敏感？
				
			- LightGBM


##### 4. 聚类方法
1）k-means
	- 原理
	- 与 EM 算法的关系
	- k 怎么确定
	- 缺点及改进

参考：
- [k-means 聚类算法](http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html)

##### 5. 最大熵模型

##### 6. 正则
- 为什么正则化可以防止过拟合？
- L1 正则为什么可以把系数压缩成 0？如何解决 L1 求导困难？
- L0.5 正则可以获得稀疏解么？L1.5 正则呢？


##### 7. 优化方法
1）梯度下降
	- 梯度下降的种类
		- 批量梯度下降
		- 随机梯度下降
		- mini-batch 梯度下降
	- 梯度下降算法的优缺点
	- 为什么负梯度方向是函数值下降最快的方向？
	- 随机梯度下降为什么能够收敛到最小值点？
	
2）牛顿法
	- 牛顿法
	- 拟牛顿法
	
3）坐标下降法



参考资料：

1）[为什么梯度反方向是函数值下降最快的方向？](https://zhuanlan.zhihu.com/p/24913912)

##### 8. 机器学习评价指标
1）假设加了很多负例，AUC 和 PRC 哪个对此不敏感？（实际上 AUC 不敏感，但也有人说 PRC 比 AUC 在极端不平衡时
曲线差很多，所以用 PRC 能反映真实的效果）

参考资料：

1）[精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？](https://www.zhihu.com/question/30643044)

##### 9. 机器学习理论
1）VC 维

2）偏差-方差分解

3）什么是过拟合？为什么会过拟合？怎么判断过拟合？怎么解决过拟合？

4）判别模型和生成模型？

### 机器学习应用
1）数据清洗及预处理
- 缺失值填充方法？
- 为什么要做归一化？哪些模型要做归一化？哪些不用？归一化的方式？
- 数据不平衡问题怎么处理？

2）特征工程

3）特征选择

4）过拟合的解决方法

5）树形结构，SVM，LR 都适用什么场景？Boosting，随机森林适用于什么场景？





##### 其他问题
- 有一堆已经分好的词，如何发现新词？

用这个词和左右词的关系。互信息 新词的左右比较丰富，有的老词的左右也比较丰富。还要区分出新词和老词。

参考：[反作弊基于左右信息熵和互信息的新词挖掘](https://zhuanlan.zhihu.com/p/25499358?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)


